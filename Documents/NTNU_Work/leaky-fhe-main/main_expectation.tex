\documentclass{llncs}[11pt]
\usepackage{fullpage}

%\pagestyle{plain}
%\usepackage{amsmath,amsfonts,amssymb,amsthm}

\let\proof\relax
\let\endproof\relax
\let\definition\relax
\let\enddefinition\relax
\let\lemma\relax
\let\endlemma\relax
\let\corollary\relax
\let\endcorollary\relax

\usepackage{amsthm} %NEW

\usepackage{xspace,amsmath,amsfonts,amssymb,afterpage,mathtools}
\usepackage[dvipsnames]{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{fixltx2e}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}

\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{csquotes}
\usepackage{sidecap}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[ruled]{algorithm2e}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{notation}{Notation}
\numberwithin{theorem}{section} % {subsection}




\newcommand{\C}{{\mathbb{C}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}

\renewcommand\vec{\mathbf}

\newcommand{\ket}[1]{|{#1}\rangle}
\newcommand{\bra}[1]{\langle{#1}|}
\newcommand{\braket}[2]{\<{#1}|{#2}\>}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\Norm}[1]{\left\|{#1}\right\|}

\newcommand{\pth}[1]{\left({#1}\right)}

\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\eq}[1]{(\ref{eq:#1})}
\renewcommand{\sec}[1]{Section~\ref{sec:#1}}

%ENCRYPTION SCHEME
\newcommand{\Gen}{\mathsf{Gen}}
\newcommand{\Enc}{\mathsf{Enc}}
\newcommand{\Dec}{\mathsf{Dec}}
\newcommand{\bin}{\mathsf{Dec}}
\newcommand{\SIVP}{\mathsf{SIVP}}
\newcommand{\GapSVP}{\mathsf{GapSVP}}
\newcommand{\LWE}{\mathsf{LWE}}
\newcommand{\Rql}{R_q^{\{1,\ldots, l\}}}


%\newcommand{\eps}{\varepsilon}


%DISTRIBUTION & EXPONENTIAL

%SCENARIO3
\newcommand{\EXP}[2]{e^{-\frac{\pi {#1}^2}{{#2}^2}}} % Used in scenario 3 only e^(-pi x^2/s^2)
%\newcommand{\EXP}[2]{\exp{\left(\frac{-{#1}^2}{2{#2}}\right)}}

%\newcommand{\EXPu}[3]{e^{-\frac{\pi ({#1}-{#2})^2}{{#3}^2}}} % Used in scenario 1 only e^(-pi (x-u)^2/s^2)

\newcommand{\asub}{\frac{\pi}{\ssigma^2}+\frac{\pi}{v^2}}
\newcommand{\censub}{\frac{z\ssigma^2}{v^2+\ssigma^2}}
\newcommand{\gEXP}[2]{e^{-{#1\left(r{#2}\right)^2}}}  %generic exp(-a(r-c)^2)
%\newcommand{\tEXP}[2]{e^{-\\pi{\left(r{#2}\right)^2}}}  %generic exp(-a(r-c)^2)
\newcommand{\ssigma}{s}
\newcommand{\temp}{\sigma}

%OTHERS

\newcommand{\bsub}{\frac{\mu_r}{\sigma_r^2}-\frac{\mu_e}{\sigma_e^2}+\frac{z}{\sigma_e^2}}
\newcommand{\csub}{-\frac{\mu_r^2}{2\sigma_r^2}-\frac{\mu_e^2}{2\sigma_e^2}+\frac{\mu_ez}{\sigma_e^2}-\frac{z^2}{2\sigma_e^2}}

%\newcommand{\iasub}{-\frac{1}{2v_i^2}-\frac{1}{2\sigma_i^2}}
\newcommand{\iasub}{\frac{1}{s_i^2}+\frac{1}{v_i^2}}
\newcommand{\ibsub}{\frac{u_i}{s_i^2}-\frac{\mu_i}{v_i^2}+\frac{z_i}{v_i^2}}
\newcommand{\icsub}{-\frac{u_i^2}{2v_i^2}-\frac{\mu_i^2}{2\sigma_i^2}+\frac{\mu_iz_i}{\sigma_i^2}-\frac{z_i^2}{2\sigma_i^2}}

\newcommand{\niasub}{\frac{1}{s^2}+\frac{1}{v^2}}
\newcommand{\nibsub}{\frac{z_i}{v^2}}
%\newcommand{\nicsub}{-\frac{\mu^2}{2\sigma^2}-\frac{z_i^2}{2\sigma^2}}

\newcommand{\rtio}{\frac{1+\tau^2}{\tau^2}}%\newcommand{\rtio}{\frac{1+\alpha^2}{\alpha^2}}
\newcommand{\rtniasub}{\frac{1+\alpha^2}{\alpha^2}\cdot\frac{1}{s^2}}
\newcommand{\rtnibsub}{\frac{z_i}{v^2}}
%\newcommand{\nicsub}{-\frac{\mu^2}{2\sigma^2}-\frac{z_i^2}{2\sigma^2}}

%NOTATION CHANGING
\newcommand{\xxip}{\xi'}
\newcommand{\xxi}{\xi}

\newcommand{\eps}{\varepsilon}
\newcommand{\numsamp}{\mathsf{num}}
\newcommand{\BKW}{\mathsf{BKW}}
\newcommand{\BKWR}{\mathsf{BKW}_{\mathsf{R}}}
\newcommand{\poly}{\mathsf{Poly}}
\newcommand{\ber}{\mathsf{Ber}}

\newcommand{\EX}{\mathbb{E}} %Expected value
\newcommand{\Var}{\mathrm{Var}} %Vari
\newcommand{\Cov}{\mathrm{Cov}} %Covariance

\newcommand{\sm}[1]{\mathsf{s}_{#1}}
\newcommand{\smx}[1]{\mathsf{s}_{#1}.x}
\newcommand{\smb}[1]{\mathsf{s}_{#1}.b}

\newcommand{\smp}[1]{\mathsf{s^{\text{p}}_{#1}}}
\newcommand{\smpx}[1]{\mathsf{s^{\text{p}}_{#1}.x}}
\newcommand{\smpb}[1]{\mathsf{s^{\text{p}}_{#1}.b}}

% LPN Oracle -> lpno
\newcommand{\lpno}[2]{\mathcal{O^{\mathsf{LPN}}_{\mathit{#1},#2}}(s)}
% p-biased LPN Oracle -> plpno
%\newcommand{\plpno}{\mathcal{O^{\mathsf{LPN}}_{\text{p},\eta}}}


%Notations
\newcommand{\kai}[2]{\mathlarger{\mathlarger{\chi}}_{#1, \mu}(#2)}
\newcommand{\kaip}[2]{\mathlarger{\mathlarger{\chi}}_{#1, p}(#2)}
\newcommand{\kaii}[1]{\mathlarger{\mathlarger{\chi}}_{#1}}
\newcommand{\cord}[2]{#1[#2]}
\newcommand{\innerprod}[3]{\langle #1, #2 \rangle_{#3}}
\newcommand{\sample}[1]{\left( x_{#1}, b_{#1} \right)}
\newcommand{\samplep}[1]{\left( x'_{#1}, b'_{#1} \right)}
\newcommand{\allprod}[2]{#1 \odot #2 }



\iftrue
\newcommand{\dnote}[1]{{\color{red}{\bf [{\bf Dana:} #1]}}}
\newcommand{\anote}[1]{{\color{blue}{\bf [Aria: #1]}}}
\newcommand{\hnote}[1]{{\color{ForestGreen}{\bf [Hunter: #1]}}}
\newcommand{\gnote}[1]{{\color{brown}{\bf [Huijing: #1]}}}
\newcommand{\tnote}[1]{{\color{purple}{\bf [Tom: #1]}}}
\fi


\iffalse
\newcommand{\dnote}[1]{}
\newcommand{\anote}[1]{}
\newcommand{\hnote}[1]{}
\newcommand{\gnote}[1]{}
\fi


\newcounter{myclaimctr}
\newenvironment{myclaim}{%      define a custom environment

\refstepcounter{myclaimctr}% increment the environment's counter
\medskip\noindent{\textbf{Claim \themyclaimctr .}}% or \textbf, \textit, .
}{}  %          create a vertical offset to following material
\numberwithin{myclaimctr}{section}


%% Save Space

%% Save the class definition of \subparagraph
\let\llncssubparagraph\subparagraph
%% Provide a definition to \subparagraph to keep titlesec happy
\let\subparagraph\paragraph
%% Load titlesec
\usepackage[compact]{titlesec}
%% Revert \subparagraph to the llncs definition
\let\subparagraph\llncssubparagraph

%default: \titlespacing*{\section} {0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titlespacing*{\section} {0pt}{2.2ex plus 0.7ex minus .4ex}{1.7ex plus .17ex}
%default: \titlespacing*{\subsection} {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\subsection} {0pt}{1.9ex plus 0.5ex minus .35ex}{1.0ex plus .13ex}
%default: \titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{1.5ex plus 0.34ex minus .2ex}{0.9ex plus .16ex}

%\titlespacing*{\paragraph}{0pt}{1.5ex plus 0.34ex minus .2ex}{0.9ex plus .16ex}


%\makeatletter
%\renewcommand{\parhead}[1]{\vskip0.34ex \noindent {\bfseries\boldmath\ignorespaces\dt@MaybeAddPunct{#1}}\hskip 0.9em plus 0.3em minus 0.3em}

%%%%%%%%%%%%%


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Expectation of Determinant}

\author{
}

\institute{
}


\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
**
\end{abstract}

%	\begin{thebibliography}{9}
%		\bibitem{Sho97}
%		P. W. Shor, \emph{Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer}, SIAM Journal on Computing 26, 1484--1509 (1997).
%		\bibitem{GKPV10}
%		Goldwasser, Shafi and Kalai, Yael Tauman and Peikert, Chris and Vaikuntanathan, Vinod, \emph{Robustness of the learning with errors assumption}, Tsinghua University Press, 2010.
%	\end{thebibliography}

%\bibliographystyle{splncs_alpha}
%\bibliography{crypto/abbrev0,crypto/crypto,ref}

\section{Expectation of Determinant after Approximate Hints}

The matrix $\vec{\Sigma}$ corresponds to the original covariance matrix for the LWE secret and error.
Formally, let $\vec{\Sigma}$ be an $2n \times 2n$ diagonal matrix with the first $n$ diagonal entries set to
$\sigma^2_s$, the second $n$ diagonal entries set to $\sigma^2_e$.
Further, let $\vec{\Sigma}_1$ be an $n \times n$ diagonal matrix with the diagonal entries set to
$\sigma^2_s$.
Let$\vec{\Sigma}_2$ be an $n \times n$ diagonal matrix with the diagonal entries set to
$\sigma^2_e$.

The matrix $\vec{\Sigma}_{\eps}$ corresponds to the covariance of the noise in the set of linear equations obtained on the LWE
secret $\vec{s}$ from decrypting a ciphertext.
Formally, let $\vec{\Sigma}_{\eps}$ be the $n \times n$ diagonal matrix with all $n$ diagonal entries set to $\sigma^2_{\eps}$.

The columns of matrix $\vec{H}$ correspond to the vectors $\vec{h}$ such that the approximate value of 
$\langle \vec{h}, (\vec{s}||\vec{e}) \rangle$ are released during decryption.
Formally, let $\vec{H}$ be the $n \times (2n)$ matrix whose $j$-th column has the first $n$ entries set to
$(h_j[1], \ldots, h_j[n])$. The next $n$ entries all $0$.
Further, let $\vec{H}_1$ be the $n \times n$ matrix whose $j$-th column has the first $n$ entries set to
$(h_j[1], \ldots, h_j[n])$.
\textbf{We assume each entry of $\vec{H}_1$ is independent with mean $0$ and variance $\sigma_h^2$.}
%Let $C$ be the $n \times n$ diagonal matrix whose diagonal is $(q, \ldots, q)$.
\dnote{If $H$ is a circulant matrix (e.g.~in the ring setting) this will not be the case. However, we can 
consider extending our analysis to that case.}

The transformed covariance matrix $\vec{\Sigma}'$ is as follows (using the conditional approximate
hints formula of \url{https://eprint.iacr.org/2020/292.pdf}). The dimension and lattice of the DBDD instance remain unchanged.
\[
\vec{\Sigma}' = \vec{\Sigma} - \vec{\Sigma} \vec{H}^T(\vec{H}\vec{\Sigma} \vec{H}^T + \vec{\Sigma}_{\eps})^{-1}\vec{H}\vec{\Sigma}.
\]

Our goal is to find $\det(\vec{\Sigma}')$. If we have this, then our tool can estimate the hardness of the new
DBDD instance. Instead of finding $\Sigma'$ and then $\det(\vec{\Sigma}')$ exactly, which requires inversion of a $2n \times 2n$ matrix, we will instead find (an upper bound on) the expected value of $\det(\vec{\Sigma}')$, where the expectation is
taken over the choice of the hint matrix $\vec{H}$.


Using the formula from here: \url{https://en.wikipedia.org/wiki/Matrix_determinant_lemma#Generalization} we have:
\[
\det(\vec{\Sigma}') = \frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma})}{\det(\vec{H}\vec{\Sigma} \vec{H}^T + \vec{\Sigma}_{\eps})}.
\]

First, note that
\[
\det(\vec{\Sigma}') = \frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma})}{\det((\vec{H}\vec{\Sigma} \vec{H}^T + \vec{\Sigma}_{\eps})}
= \frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma}_1)\det(\vec{\Sigma}_2)}{\det((\vec{H}\vec{\Sigma} \vec{H}^T + \vec{\Sigma}_{\eps})}
\leq
\frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma}_1)\det(\vec{\Sigma}_2)}{\det(\vec{H}\vec{\Sigma} \vec{H}^T)}
\]


We will therefore compute a lower bound on the quantity
\[
\mathbb{E}[\det(\vec{H}\vec{\Sigma} \vec{H}^T)] = \sigma_s^{2n} \det(\vec{H}_1 \vec{H}_1^T).
\]

Further, note that each coordinate of $\vec{H}_1$ is independent and has mean $0$ and variance $\sigma^2_h$.
Therefore,
$\vec{H}_1 \vec{H}_1^T = \sigma_h^2 R aR^T$, where
$R$ is a matrix where each coordinate is independent with mean $0$ and variance $1$ \dnote{Actually we just need pairwise independence of the coordinates}.
It is not hard to see that $\mathbb{E}[\det(R R^T)] = n!$.
Which implies that
$\mathbb{E}[\det(\vec{H}_1 \vec{H}_1^T)] = \sigma_h^{2n} \cdot n!$.

Thus,
\[
\mathbb{E}[\frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma}_1)\det(\vec{\Sigma}_2)}{\det(\vec{H}\vec{\Sigma} \vec{H}^T)}] =
\frac{\sigma_{\eps}^{2n} \sigma_e^{2n}}{\sigma_h^{2n} \cdot n!}.
\]

\section{The Ring LWE Case}

In this case we consider leakage
of (noisy) $\vec{s} \cdot \vec{h}$, where multiplication is over the ring $R_q$. Since we assume that
both $\vec{s}$ and $\vec{h}$ are small magnitude, there is actually no wraparound modulo $q$.
In this case, we can view the multiplication as over the ring of integers $\mathbb{Z}/\Phi(m)$, where $\Phi(m)$ is the
$m$-th cyclotomic polynomial of degree $n = \phi(m)$, and $n$ is a power of two.

First, note that for fixed $\vec{h}$ and all $\vec{s}$,
\[
\vec{s} \cdot \vec{h} = \vec{s}\vec{V} \vec{B} \vec{P} \left ( \vec{M}(h) \right ) \vec{P}^{-1} \vec{B}^{-1} \vec{V}^{-1},
\]
where $\vec{V}$ is the canonical embedding transformation, $\vec{B}$ is the matrix corresponding
to the isomorphism between $H \subset \mathbb{C}^{\mathbb{Z}^*_m}$ and $\mathbb{R}^n$, where $n = \phi(m)$, $\vec{P}$ is a permutation matrix,
and
$\vec{M}(h)$ is a block diagonal matrix with $n/2$ blocks, each of dimension $2 \times 2$, where
the $i$-th block is
\[
\begin{bmatrix}
1/\sqrt{2} v_i & 1/\sqrt{2} v_{n-i}\\
-1/\sqrt{2} v_{n-i} & 1/\sqrt{2} v_i,
\end{bmatrix}
\]
and $\vec{v} = (v_1, \ldots, v_n)$ is equal to $\vec{v} = \vec{h} \vec{V} \vec{B} \vec{P}$.
Since $\vec{V} \vec{B} \vec{P}$
is an isometry (an orthogonal matrix scaled by $\sqrt{n}$),
we have that $\sigma^2_h (\vec{V}\vec{B}\vec{P})(\vec{V}\vec{B}\vec{P})^T = n \sigma^2_h \cdot \vec{I}_n$.
So
for each  $i \in [n/2]$, $v_i$ and $v_{n-i}$ are distributed as independent Gaussians with variance 
$n \sigma^2_h$.
Note that $(\vec{V}\vec{B}\vec{P})$ is a \emph{real} matrix, even though $\vec{V}$ and $\vec{B}$
themselves are complex.

The expected determinant of $\vec{M}(h)$ is therefore equal to
\begin{align*}
\prod_{i \in [n/2]} \mathbb{E}[1/2 v^2_i + 1/2v^2_{n-i}] &= (n \sigma_h^2)^{n/2}\\
&= (\sqrt{n} \sigma_h)^n
\end{align*}

Further, by Jensen's ineqaulity, the expected determinant of
$\vec{M}(h) (\vec{M}(h))^T$
is at least $n^n \cdot \sigma^{2n}_h$.
\dnote{Compare this with $\sigma_h^{2n} \cdot n!$ from the previous case.}

\dnote{Added.}
For a single hint on $(\vec{s}||\vec{e})$:
\[
\det(\vec{\Sigma}') \leq \frac{\det(\vec{\Sigma}_{\eps})\det(\vec{\Sigma})}{n^n (\sigma^2_{s}\sigma^2_{h, s} + \sigma^2_e\sigma^2_{h, e})^n},
\]
where $\sigma^2_{h,e}$ is the variance of the coordinates of the $\vec{v}$ vector
from Ana's writeup, $\sigma^2_{h,s}$ is the variance of the coordinates of the $\vec{e}_2$
vector from Ana's writeup.

Given $\ell \geq 4$ hints, I expect the result to be:
\[
\det(\vec{\Sigma}') \leq \frac{\sigma^{2n}_{\eps}\det(\vec{\Sigma})}{n^n (\sigma^2_{s}\sigma^2_{h, s}(\ell - 4\ln(\ell)) \sigma^2_e\sigma^2_{h, e}\ell)^{n/2}},
\]

\end{document}
